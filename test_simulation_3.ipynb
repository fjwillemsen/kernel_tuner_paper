{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Tuner Simulation Mode Test 3\n",
    "\n",
    "This notebook is intended to examine the simulation mode against the normal mode using a fixed seed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import warn\n",
    "from typing import Tuple\n",
    "\n",
    "cachedirectory = Path(\"kernels/outputdata/simulation_mode\")\n",
    "savedirectory = Path(Path.cwd(), \"figures\", \"simulation_mode\")\n",
    "mpl.rcParams[\"savefig.directory\"] = savedirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "kernelname = \"gemm_cltune_opencl\"\n",
    "expected_keys_env = ['execution_time', 'overhead_time', 'total_benchmark_time', 'total_verification_time', 'total_compile_time', 'total_strategy_time', 'total_framework_time']\n",
    "expected_keys_cache = ['time', 'times', 'GFLOP/s', 'temperature', 'compile_time', 'benchmark_time', 'verification_time', 'framework_time', 'strategy_time']\n",
    "\n",
    "algorithm_displaynames = {\n",
    "    'brute': \"bruteforce\",\n",
    "    'random': \"random\",\n",
    "    'ordered': \"ordered_greedy_mls\",\n",
    "}\n",
    "column_displaynames = {\n",
    "    'compile_time': \"Compile time\",\n",
    "    'benchmark_time': \"Benchmark time\",\n",
    "    'verification_time': \"Verification time\",\n",
    "    'strategy_time': \"Strategy time\",\n",
    "    'framework_time': \"Framework time\",\n",
    "    'total_time': \"Total time\"\n",
    "}\n",
    "\n",
    "plot_height = 6\n",
    "plot_aspect = 2.0\n",
    "plot_dpi = 250\n",
    "sns.set_theme(rc={'figure.figsize':(plot_height*plot_aspect,plot_height)}, font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes(target_algorithm=None, fixed_fevals=False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # collect the relevant cachefiles\n",
    "    cachefiles : list[Path] = []\n",
    "    for file in cachedirectory.iterdir():\n",
    "        if file.is_file() and file.name.startswith(kernelname) and file.name.endswith('_output.json'):\n",
    "            cachefiles.append(file)\n",
    "\n",
    "    # create a list of dictionaries of the data\n",
    "    cache_dicts = list()\n",
    "    env_dicts = list()\n",
    "    for filepath in cachefiles:\n",
    "        # load the data\n",
    "        envfilepath = Path(str(filepath).replace('_output', '_env'))\n",
    "        assert envfilepath.exists()\n",
    "        try:\n",
    "            cachefile = json.load(filepath.open())\n",
    "            envfile = json.load(envfilepath.open())\n",
    "        except json.JSONDecodeError:\n",
    "            warn(f\"Could not load cachefile {filepath.name}\")\n",
    "            continue\n",
    "        assert isinstance(cachefile, list)\n",
    "        assert isinstance(envfile, dict)\n",
    "\n",
    "        # combine the data into a dictionary\n",
    "        device_name = envfile['device_name']\n",
    "        is_simulated = \"_mode=simulated\" in filepath.name\n",
    "        mode = 'simulated' if is_simulated else 'real'\n",
    "        algorithm = algorithm_displaynames[re.search(r\"_alg=([a-zA-Z]+)_\", filepath.name).group(1).strip()]\n",
    "        if target_algorithm is not None and not algorithm.startswith(target_algorithm):\n",
    "            continue\n",
    "        if fixed_fevals != ('_fixedfevals' in filepath.name):\n",
    "            continue\n",
    "        run_number = int(re.search(r\"_#(\\d+)_\", filepath.name).group(1))\n",
    "        # for each of the tuned configurations, create a dictionary with the expected items\n",
    "        for value in cachefile:\n",
    "            assert isinstance(value, dict)\n",
    "            cache = {}\n",
    "            cache['device_name'] = device_name\n",
    "            cache['algorithm'] = algorithm\n",
    "            cache['simulated'] = is_simulated\n",
    "            cache['mode'] = mode\n",
    "            cache['run_number'] = run_number\n",
    "            for key in expected_keys_cache:\n",
    "                cache[key] = value[key]\n",
    "            cache['total_time'] = value['strategy_time'] + value['compile_time'] + value['benchmark_time'] + value['verification_time'] + value['framework_time']\n",
    "            cache_dicts.append(cache)\n",
    "        # create a global dictionary per tuning session\n",
    "        env = {}\n",
    "        env['device_name'] = device_name\n",
    "        env['algorithm'] = algorithm\n",
    "        env['simulated'] = is_simulated\n",
    "        env['mode'] = mode\n",
    "        env['number_of_configs'] = len(cachefile)\n",
    "        env['run_number'] = run_number\n",
    "        for key in expected_keys_env:\n",
    "            env[key] = envfile[key]\n",
    "        env_dicts.append(env)\n",
    "\n",
    "    # create a dataframe out of the dictionaries\n",
    "    df_cache = pd.DataFrame(cache_dicts)\n",
    "    df_env = pd.DataFrame(env_dicts)\n",
    "    return df_cache, df_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cache, df_env = get_dataframes(target_algorithm='ordered')\n",
    "df_cache"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

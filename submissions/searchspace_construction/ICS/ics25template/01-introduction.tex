\section{Introduction}
\label{sec:introduction}

Graphics Processing Units (GPUs) have revolutionized the computing landscape in the past decade, providing previously unattainable computational performance for compute-intensive tasks such as artificial intelligence and climate simulation~\cite{heldens2020landscape,lecun2015deep}. 
Nine out of the top ten supercomputers in the TOP 500 listing of November 2024 use GPUs as the main source of compute power, and systems with accelerators account for 82.6\% of the combined TOP 500 RMax performance~\cite{TOP500November2024}.   % 209 top500 systems use accelerators, but the systems with accelerators account for (9685074.920 / 11723130.436) = 82.615% of RMax (HPL benchmark performance)
GPUs excel in terms of compute performance and energy efficiency for tasks that involve large data sets and dense computation, making them increasingly vital in various scientific domains~\cite{lessonsLearnedGPU2020}. 
In the past decade, GPUs have become increasingly complex computing devices with larger register files, more specialized cores, and larger and more complex streaming multiprocessors (SMs), while also dramatically increasing the number of SMs per chip~\cite{hijma2023optimization}. 
In addition, energy efficiency and accuracy play an increasingly important role in the auto-tuning of GPU applications~\cite{schoonhovenGoingGreenOptimizing2022, heldensKernelLauncherLibrary2023}. 

GPU programming models, such as CUDA, HIP, and OpenCL, allow developers to create highly parallel functions, called {\em kernels}, that run on the GPU. 
Developers are confronted with a myriad of implementation choices and optimization techniques related to thread organization, memory usage, and computation strategies to achieve optimal compute performance~\cite{hijma2023optimization}. 
Many different design choices have a substantial and hard-to-predict impact on the performance, energy efficiency, and accuracy of GPU kernels, as the optimal kernel configuration depends on a complex interplay of hardware, device software, and the program itself.
This optimization problem leads to an overwhelming number of code variants if done manually, spurring the creation of frameworks that facilitate automatic performance tuning, or {\em auto-tuning}, to automatically tune GPU kernels and related software~\cite{OpenTuner, CLTune, KTT, ATF, vanwerkhovenKernelTunerSearchoptimizing2019}.

As a consequence of the widespread adoption of GPUs for computation, increased complexity of GPUs, and improvements in auto-tuning, the number of parameters to be tuned is increasing, as well as the range of values per parameter. This leads to a large number of possible combinations (the \emph{Cartesian} product) and is reflected in auto-tuned GPU applications, with the number of valid kernel code variants, or {\em configurations}, per search space at millions and approaching billions in recent work~\cite{BenchmarkingSuiteKerneltuners, heldensKernelLauncherLibrary2023,BaCO2024}. 

% The dramatically increased search space size creates new challenges for auto-tuning GPU applications. 
% Auto-tuning frameworks and their users employ \emph{constraints} on the tunable parameters to avoid attempts to tune invalid or non-sensical configurations, for example, an invalid product of thread block sizes.
% As such, creating and representing the search space itself, with billions of possible combinations to resolve in a high-dimensional, discontinuous space quickly becomes a bottleneck at the start of the tuning process~\cite{ATF2}. 
In auto-tuning, the collection of all possible combinations of all parameter values tends to contain many configurations that are not valid. For example, because the product of the thread block sizes can not be larger than some hardware limitation, or because some combination of parameter values would lead to incorrect results in the program. To filter out such configurations, constraints are specified on combinations of tunable parameter values. 
The dramatically increased search space size creates new challenges for auto-tuning frameworks, as with possibly billions of code variants to enumerate in a high-dimensional space, where each constraint must be checked on each variant to resolve the validity, constructing the search space can become a bottleneck at the start of the tuning process. 
This is observed for several real-world tunable applications, where the search space construction time can take several minutes, or even days (measured per the experimental setup described in \cref{sec:evaluation}). This is time that could have been spent on tuning, but is instead lost to the overhead of constructing the search space. % ; for example, the brute force search space construction of the Hotspot kernel used in \cref{sec:evaluation} takes over 8 minutes, during which the GPU is not used and the user receives no tangible result or feedback.  % or 486.517 seconds % optimized method takes 1.514 seconds | ATF PRL takes 64098.0263 seconds vs 0.237 seconds
% \todo[inline]{Give context for 8 minutes search space construction}
% Eight minutes may not seem like a  \todo[inline]{continue here}. 

% To address this issue, this work introduces the use of Constraint Satisfaction Problem (CSP) solvers and various optimizations for constructing auto-tuning search spaces, dramatically improving over the state-of-the-art in search space construction for auto-tuning.
% In particular, we examine and evaluate various solver techniques and implementations to decide which approach is best suited for auto-tuning, and greatly optimize the best approach by creating C-extensions, efficiently parsing inputs, extending and optimizing built-in constraints, and efficiently representing the resulting search space.
To address this, we examine and evaluate various solver techniques and implementations to decide which approach is best suited for auto-tuning, and greatly optimize the best approach, dramatically improving over the state-of-the-art in search space construction for auto-tuning.
In particular, we: 
\begin{itemize}
    \item Introduce the use of Constraint Satisfaction Problem (CSP) solvers for constructing auto-tuning search spaces.
    \item Optimize an algorithm for the auto-tuning domain.
    \item Extend built-in constraints for cases common in auto-tuning.
    \item Optimize built-in constraints using preprocessing and dynamic runtime compilation. 
    \item Parse general user inputs to subsets of built-in constraints.
    \item Create C-extensions to improve performance. 
    \item Provide various output formats to avoid rearrangement. 
    \item Represent and index the resulting search space for efficient exploration and navigation during tuning.
\end{itemize}
Our contributions have been implemented in python-constraint~\footnote{\url{https://github.com/python-constraint/python-constraint}} and Kernel Tuner~\cite{vanwerkhovenKernelTunerSearchoptimizing2019}~\footnote{\url{https://github.com/KernelTuner/kernel_tuner}}, both available as open-source packages, enabling straightforward adoption by the auto-tuning community and related fields.

The remainder of this work is structured as follows.
Section~\ref{sec:related_work} discusses related work.
% Section~\ref{sec:impl} presents the design and implementation of our new contributions in Kernel Tuner.
Section~\ref{sec:searchspace_construction} describes the context, selection, optimization, and implementation of search space solvers for constructing and representing auto-tuning search spaces.
In Section~\ref{sec:evaluation}, we evaluate the efficiency and scalability of our optimized CSP-based approach against various other state-of-the-art solutions on a wide variety of synthetic and real-world search spaces, demonstrating the orders-of-magnitude improvements in performance. 
Section~\ref{sec:conclusion_futurework} concludes this work.
